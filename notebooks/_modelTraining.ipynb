{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "#logging.getLogger('').handlers = []  #To delete previous logging configuration\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO,\n",
    "    #If log into standard output and to a file is desired:\n",
    "    #handlers=[logging.FileHandler(\"{0}/{1}.log\".format('./', uniName)), #path, File name\n",
    "    #         logging.StreamHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from word2vecTraining import preprocessing, training, evaluation\n",
    "\n",
    "# Where Text Corpus is located\n",
    "inputPath = os.path.normpath(r'D:\\data\\nyt') \n",
    "#inputPath = os.path.normpath(r'.') \n",
    "\n",
    "# Get all CSV Text Corpus files In target Path\n",
    "#onlyfiles = [f for f in listdir(inputPath) if isfile(join(inputPath, f)) and f.endswith('.csv')]\n",
    "#onlyfiles = onlyfiles[:2] #For testing\n",
    "# print(onlyfiles)\n",
    "# len(onlyfiles)\n",
    "\n",
    "#Where output word2vec models will be stored\n",
    "outputPath = os.path.normpath(r'D:\\data\\nyt\\word2vecModels') #\\ to \\\\\n",
    "#outputPath = os.path.normpath(r'.\\models') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the processing of 2001\n",
      "Reading Corpus file and preprocessing time:74.14247250556946 seconds\n",
      " printing Top 2 and last sentences For sanity check\n",
      "0 ['drug', 'explosives', 'detection', 'equipment', 'maker', 'barringer', 'technologies', 'said', 'yesterday', 'agreed', 'acquired', 'diversified', 'manufacturer', 'smiths', 'group', 'million', 'barringer', 'said', 'agreed', 'acquired', 'smiths', 'share', 'represents', 'percent', 'premium', 'barringer', 'shares', 'based', 'thursday', 'closing', 'stock', 'price', 'share', 'separately', 'barringer', 'reported', 'surge', 'fourth', 'quarter', 'net', 'income', 'cents', 'diluted', 'share', 'cents', 'share', 'year', 'earlier', 'revenue', 'rose', 'million', 'million']\n",
      "1 ['nanny', 'upper', 'east', 'work', 'schedule', 'cut', 'days', 'week', 'wife', 'household', 'lost', 'job', 'wall', 'street', 'limousine', 'drivers', 'complain', 'carrying', 'fewer', 'passengers', 'earning', 'percent', 'restaurants', 'waiters', 'working', 'days', 'week', 'instead', 'bellhops', 'manhattan', 'star', 'hotels', 'tips', 'account', 'half', 'income', 'plummeted', 'half', 'workers', 'huge', 'largely', 'overlooked', 'segment', 'new', 'york', 'city', 'ailing', 'economy', 'estimated', 'economists', 'number', 'workers', 'suffered', 'badly', 'sept', 'terrorist', 'attack', 'world', 'trade', 'center', 'workers', 'separate', 'thrown', 'work', 'attack', 'according', 'state', 'figures', 'experienced', 'painful', 'drop', 'income', 'holding', 'jobs', 'working', 'fewer', 'hours', 'carrying', 'fewer', 'passengers', 'receiving', 'lower', 'tips', 'economists', 'assessing', 'ripple', 'effects', 'sept', 'attack', 'originally', 'focused', 'surge', 'unemployment', 'beginning', 'examine', 'large', 'group', 'workers', 'studying', 'shrunken', 'incomes', 'add', 'new', 'york', 'steep', 'economic', 'losses', 'financial', 'troubles', 'faced', 'workers', 'far', 'reverse', 'good', 'times', 'low', 'income', 'workers', 'experienced', 'past', 'decade', 'despite', 'economic', 'suffering', 'workers', 'fail', 'qualify', 'assistance', 'declining', 'incomes', 'cascading', 'effect', 'hurting', 'retailers', 'landlords', 'alike', 'level', 'hardship', 'captured', 'employment', 'data', 'including', 'unemployment', 'numbers', 'said', 'mark', 'levitan', 'senior', 'policy', 'analyst', 'community', 'service', 'society', 'new', 'york', 'going', 'fiscal', 'impact', 'tax', 'money', 'going', 'city', 'treasury', 'going', 'severe', 'impact', 'low', 'income', 'neighborhoods', 'like', 'downtown', 'brooklyn', 'street', 'significant', 'gains', 'good', 'years', 'nazir', 'khan', 'limousine', 'driver', 'complains', 'plunge', 'business', 'company', 'receiving', 'fewer', 'calls', 'pick', 'goldman', 'sachs', 'morgan', 'stanley', 'bankers', 'passengers', 'airways', 'shuttle', 'weekly', 'gross', 'fallen', 'pays', 'week', 'gas', 'insurance', 'car', 'payments', 'wealthy', 'passengers', 'khan', 'said', 'grown', 'tightfisted', 'fewer', 'people', 'giving', 'tips', 'far', 'said', 'let', 'average', 'day', 'tips', 'lot', 'financially', 'squeezed', 'workers', 'limousine', 'drivers', 'butlers', 'housekeepers', 'waiters', 'nannies', 'bellhops', 'hurt', 'wholesale', 'belt', 'tightening', 'city', 'wealthy', 'new', 'yorkers', 'badly', 'shaken', 'sept', 'subsequent', 'economic', 'slump', 'surprisingly', 'cut', 'spending', 'new', 'york', 'like', 'london', 'global', 'cities', 'luxury', 'service', 'class', 'relied', 'healthy', 'upper', 'class', 'said', 'joshua', 'freeman', 'historian', 'queens', 'college', 'upper', 'class', 'catches', 'cold', 'rely', 'catch', 'pneumonia', 'khan', 'prime', 'example', 'ends', 'meet', 'works', 'hour', 'days', 'midnight', 'instead', 'hours', 'sept', 'told', 'teenagers', 'forget', 'buying', 'latest', 'nike', 'styles', 'hottest', 'advising', 'buy', 'brands', 'buy', 'need', 'hardest', 'period', 'said', 'khan', 'immigrated', 'guyana', 'years', 'ago', 'luis', 'molina', 'taxi', 'driver', 'puerto', 'rico', 'faces', 'similar', 'squeeze', 'said', 'financial', 'district', 'business', 'fallen', 'helping', 'drag', 'daily', 'gross', 'pays', 'day', 'lease', 'cab', 'slew', 'stockbrokers', 'molina', 'said', 'good', 'tippers', 'lot', 'brought', 'mistresses', 'want', 'nearest', 'hotel', 'changed', 'think', 'stopped', 'partying', 'taxi', 'limousine', 'drivers', 'city', 'making', 'slightly', 'half', 'sept', 'working', 'longer', 'hours', 'add', 'thousands', 'waiters', 'bellhops', 'nannies', 'suffering', 'shrunken', 'paychecks', 'economists', 'estimate', 'people', 'group', 'conservative', 'workers', 'complain', 'slipping', 'safety', 'net', 'unable', 'qualify', 'aid', 'like', 'unemployment', 'insurance', 'usually', 'half', 'weekly', 'pay', 'group', 'complains', 'unemployed', 'easier', 'time', 'obtaining', 'government', 'health', 'insurance', 'like', 'medicaid', 'safety', 'net', 'isn', 'designed', 'gray', 'area', 'levitan', 'said', 'new', 'twilight', 'zone', 'people', 'formally', 'employed', 'work', 'hours', 'unemployment', 'insurance', 'incomes', 'high', 'public', 'assistance', 'food', 'stamps', 'incomes', 'hurt', 'severely', 'mike', 'wallace', 'author', 'gotham', 'pulitzer', 'winning', 'history', 'new', 'york', 'city', 'sees', 'parallels', 'today', 'downturn', 'great', 'depression', 'humbled', 'financiers', 'fired', 'chauffeurs', 'butlers', 'laundrywomen', 'fewer', 'personal', 'chauffeurs', 'nowadays', 'said', 'wealthy', 'instead', 'cutting', 'use', 'limousine', 'companies', 'wallace', 'said', 'cutting', 'rich', 'rich', 'merely', 'rich', 'upper', 'middle', 'class', 'roseana', 'thomas', 'nanny', 'upper', 'east', 'recently', 'saw', 'schedule', 'cut', 'days', 'week', 'reducing', 'weekly', 'pay', 'noting', 'husband', 'wife', 'lost', 'jobs', 'said', 'worries', 'downturn', 'evidently', 'persuaded', 'trim', 'spending', 'teenage', 'daughters', 'home', 'brooklyn', 'monthly', 'rent', 'thomas', 'said', 'share', 'apartment', 'work', 'soon', 'economize', 'laundry', 'bathtub', 'instead', 'coin', 'laundry', 'baking', 'cooking', 'lot', 'instead', 'buying', 'prepared', 'foods', 'hard', 'times', 'fueling', 'tensions', 'teenagers', 'believe', 'talking', 'letting', 'know', 'reality', 'told', 'better', 'think', 'santa', 'visiting', 'said', 'primary', 'business', 'royal', 'airline', 'laundry', 'washing', 'linens', 'airlines', 'percent', 'sept', 'having', 'laid', 'fourth', 'workers', 'plant', 'far', 'rockaway', 'queens', 'sherwin', 'glabman', 'royal', 'chief', 'executive', 'said', 'seen', 'like', 'iraqi', 'war', 'côte', 'basque', 'restaurant', 'street', 'lost', 'business', 'including', 'corporate', 'banquets', 'result', 'ricardo', 'cerdeira', 'banquet', 'captain', 'worked', 'days', 'week', 'weeks', 'working', 'preceding', 'month', 'mid', 'september', 'mid', 'october', 'cerdeira', 'laid', 'plunge', 'business', 'sept', 'september', 'december', 'traditionally', 'best', 'time', 'new', 'york', 'restaurant', 'industry', 'cerdeira', 'said', 'november', 'things', 'haven', 'gotten', 'better', 'january', 'february', 'march', 'usually', 'worst', 'months', 'industry', 'lot', 'fear', 'happens', 'palace', 'hotel', 'bellhops', 'laid', 'mid', 'september', 'stayed', 'agreed', 'union', 'suggestion', 'day', 'work', 'week', 'minimize', 'number', 'layoffs', 'brendan', 'daly', 'said', 'lose', 'percent', 'pay', 'giving', 'day', 'work', 'tips', 'fell', 'percent', 'hotel', 'vacancy', 'rate', 'soared', 'generosity', 'clients', 'plummeted', 'good', 'times', 'tips', 'run', 'week', 'wages', 'weekly', 'income', 'bellhops', 'plunged', 'tips', 'basically', 'butter', 'bread', 'said', 'butter', 'running', 'little', 'dry']\n",
      "85903 ['best', 'honor', 'fallen', 'heroes', 'recalling', 'causes', 'fought', 'eschewing', 'smug', 'triumphalism', 'counting', 'current', 'blessings', 'american', 'freedoms', 'remember', 'victory', 'ensures', 'government', 'consent', 'governed', 'fretting', 'unvoted', 'switch', 'power', 'senate', 'wondering', 'set', 'agenda', 'parties', 'conflict', 'compromise', 'recent', 'election', 'electorate', 'split', 'middle', 'ideology', 'won', 'branches', 'congress', 'white', 'house', 'supreme', 'court', 'conservative', 'hands', 'narrowest', 'margins', 'constitutionally', 'legitimate', 'sided', 'result', 'photo', 'finish', 'race', 'struck', 'whack', 'better', 'reflection', 'consent', 'undecided', 'governed', 'closely', 'divided', 'government', 'result', 'branches', 'choice', 'paralysis', 'media', 'blame', 'casting', 'progress', 'political', 'horse', 'trading', 'voters', 'year', 'punish', 'incumbents', 'parties', 'partisan', 'gridlock', 'public', 'officials', 'incentive', 'perfect', 'art', 'deal', 'compromise', 'order', 'day', 'principled', 'struggles', 'party', 'choose', 'fight', 'sacrifice', 'governed', 'demonstrably', 'uncertain', 'getting', 'consented', 'ingenious', 'blessing', 'economic', 'philosophy', 'produced', 'prosperity', 'wonder', 'world', 'communism', 'socialism', 'turned', 'wavelets', 'past', 'fascism', 'corporate', 'state', 'failed', 'vestiges', 'persist', 'benighted', 'authoritarian', 'regimes', 'hidden', 'hands', 'free', 'market', 'economics', 'applaud', 'american', 'locomotive', 'pulling', 'world', 'forward', 'capitalist', 'raises', 'standard', 'living', 'won', 'worldwide', 'competition', 'business', 'cycle', 'natural', 'breathing', 'temporary', 'downturn', 'short', 'term', 'shortage', 'brings', 'cries', 'veer', 'command', 'economy', 'demagogic', 'energy', 'price', 'caps', 'politically', 'satisfying', 'populist', 'interference', 'market', 'self', 'correction', 'lead', 'worse', 'shortages', 'rationing', 'inflation', 'wage', 'control', 'high', 'prices', 'invite', 'fresh', 'supply', 'restrict', 'demand', 'lowers', 'prices', 'government', 'ameliorate', 'pain', 'transition', 'enforce', 'fair', 'competition', 'unshackled', 'working', 'enterprise', 'best', 'balances', 'supply', 'demand', 'proved', 'practical', 'known', 'humanity', 'personal', 'freedom', 'built', 'grateful', 'america', 'exemplar', 'emphasis', 'economic', 'freedom', 'source', 'military', 'strength', 'possible', 'lead', 'fight', 'save', 'democracy', 'strength', 'coupled', 'long', 'held', 'vision', 'human', 'rights', 'blesses', 'foreign', 'policy', 'moral', 'force', 'worthy', 'mission', 'preserve', 'defend', 'freedom', 'national', 'selflessly', 'flowers', 'remembrance', 'laid', 'american', 'graves', 'world', 'memorial', 'day', 'zeal', 'rights', 'human', 'beings', 'free', 'scorned', 'europeans', 'like', 'reminded', 'saved', 'enslavement', 'france', 'thomas', 'paine', 'wrote', 'rights', 'man', 'polemic', 'soon', 'vindicated', 'men', 'women', 'england', 'mary', 'wollstonecraft', 'outspoken', 'espousal', 'human', 'rights', 'seen', 'manifestation', 'hegemony', 'unwelcome', 'hyperpower', 'safe', 'healthy', 'nations', 'freed', 'nazism', 'saved', 'communism', 'words', 'post', 'depression', 'republicans', 'throwing', 'crutches', 'doctor', 'comes', 'obligatory', 'sure', 'graph', 'consent', 'governed', 'leaves', 'dissenting', 'minority', 'unsatisfied', 'free', 'market', 'needs', 'protection', 'competition', 'privacy', 'public', 'health', 'safety', 'net', 'people', 'compete', 'nation', 'stand', 'human', 'rights', 'decorating', 'resting', 'places', 'america', 'fighters', 'remember', 'legacy', 'freedoms', 'charge', 'day', 'pay', 'personal', 'respects', 'national', 'pride']\n",
      " stats about Corpus read from file\n",
      "Number of words in corpus: 26454392\n",
      "Number of sentences in corpus: 85903\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import time\n",
    "from gensim.parsing.preprocessing import * #provides a number of convenience preprocessing functions optimized for speed\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(file):\n",
    "\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), #To lowercase\n",
    "                      lambda text: re.sub(r'https?:\\/\\/.*\\s', '', text, flags=re.MULTILINE), #To Strip away URLs\n",
    "                      #split_alphanum, #Add spaces between digits & letters in s using RE_AL_NUM.\n",
    "                      strip_tags, #Remove tags from s using RE_TAGS.\n",
    "                      strip_non_alphanum,#Remove non-alphabetic characters from s using RE_NONALPHA.\n",
    "                      strip_punctuation, #Replace punctuation characters with spaces in s using RE_PUNCT.\n",
    "                      strip_numeric, #Remove digits from s using RE_NUMERIC.\n",
    "                      strip_multiple_whitespaces,#Remove repeating whitespace characters (spaces, tabs, line breaks) from s and turns tabs & line breaks into spaces using RE_WHITESPACE.\n",
    "                      remove_stopwords, # Set of 339 stopwords from Stone, Denis, Kwantes (2010).\n",
    "                      #lambda x:\" \".join(w for w in x.split() if w not in stopword_file) #Custom stopwords\n",
    "                      lambda x: strip_short(x, minsize=3), #Remove words with length lesser than minsize from s.\n",
    "                      #stem_text #Transform s into lowercase and stem it.\n",
    "                     ]\n",
    "\n",
    "    tic = time.time() # Start timing\n",
    "\n",
    "\n",
    "    csv.field_size_limit(2147483647)\n",
    "    #csv.field_size_limit(sys.maxsize)\n",
    "    #Option 1: split paragraphs Into  sentences\n",
    "    with open(file,'r', newline='',encoding=\"utf-8\") as inpFile:\n",
    "\n",
    "        csvObject = csv.reader(inpFile, delimiter=',',quotechar='\"')\n",
    "\n",
    "        wordThreshold=5 #Important: filter out sentences with less than wordThreshold words\n",
    "\n",
    "        sentences = []\n",
    "        for csvEntry in csvObject:\n",
    "            if len(csvEntry)>1:\n",
    "                #IMPORTANT: If all of your sentences have been loaded as one sentence, Word2vec training could take a very long time.\n",
    "                #That’s because Word2vec is a sentence-level algorithm, so sentence boundaries are very important, because\n",
    "                #co-occurrence statistics are gathered sentence by sentence. For many corpora, average sentence length is six words.\n",
    "                #That means that with a window size of 5 you have, say, 30 (random number here) rounds of skip-gram calculations.\n",
    "                #If you forget to specify your sentence boundaries, you may load a “sentence” that’s 10,000 words long.\n",
    "                #In that case, Word2vec would attempt a full skip-gram cycle for the whole 10,000-word “sentence”. Hence, I split\n",
    "                #the CSV entries By paragraphs '\\n\n",
    "                lines = csvEntry[0].split('\\n') #csvEntry[0] is url csvEntry[1] is text Fetched from URL\n",
    "\n",
    "                for line in lines: #Different elements appear in their own line\n",
    "                    words = preprocess_string(line,CUSTOM_FILTERS)\n",
    "\n",
    "                    if len(words)>wordThreshold: #Important: filter out sentences with less than wordThreshold words\n",
    "                        sentences.append(words)\n",
    "\n",
    "    toc = time.time() # Start timing\n",
    "    computationTime = toc-tic\n",
    "\n",
    "    print(\"Reading Corpus file and preprocessing time:\" +str(computationTime)+\" seconds\")\n",
    "\n",
    "    print(\" printing Top 2 and last sentences For sanity check\")\n",
    "    for i, s in enumerate(sentences[0:2]):\n",
    "        print(i,s)\n",
    "    print(len(sentences),sentences[-1])\n",
    "\n",
    "    print(\" stats about Corpus read from file\")\n",
    "    wordsInCorpus = 0\n",
    "    for i, s in enumerate(sentences):\n",
    "        wordsInCorpus += len(s)\n",
    "    print(\"Number of words in corpus:\",wordsInCorpus)\n",
    "    print(\"Number of sentences in corpus:\",len(sentences))\n",
    "    #for i, s in enumerate(sentences[0:30]):\n",
    "    #    print(i,s)\n",
    "    return sentences\n",
    "\n",
    "year = '2001'\n",
    "\n",
    "\n",
    "print(\"starting the processing of \" + year)\n",
    "inputFilePath = os.path.join(inputPath, year+'.csv')\n",
    "sentences = preprocessing(inputFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-15 11:38:38,000 : INFO : collecting all words and their counts\n",
      "2018-10-15 11:38:38,001 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-10-15 11:38:38,520 : INFO : PROGRESS: at sentence #10000, processed 3069108 words, keeping 90254 word types\n",
      "2018-10-15 11:38:39,121 : INFO : PROGRESS: at sentence #20000, processed 6464838 words, keeping 126750 word types\n",
      "2018-10-15 11:38:39,651 : INFO : PROGRESS: at sentence #30000, processed 9542519 words, keeping 148851 word types\n",
      "2018-10-15 11:38:40,197 : INFO : PROGRESS: at sentence #40000, processed 12697773 words, keeping 167860 word types\n",
      "2018-10-15 11:38:40,750 : INFO : PROGRESS: at sentence #50000, processed 15824708 words, keeping 185342 word types\n",
      "2018-10-15 11:38:41,274 : INFO : PROGRESS: at sentence #60000, processed 18758872 words, keeping 201594 word types\n",
      "2018-10-15 11:38:41,803 : INFO : PROGRESS: at sentence #70000, processed 21710308 words, keeping 217517 word types\n",
      "2018-10-15 11:38:42,334 : INFO : PROGRESS: at sentence #80000, processed 24653636 words, keeping 230406 word types\n",
      "2018-10-15 11:38:42,657 : INFO : collected 237658 word types from a corpus of 26454392 raw words and 85903 sentences\n",
      "2018-10-15 11:38:42,658 : INFO : Loading a fresh vocabulary\n",
      "2018-10-15 11:38:43,382 : INFO : effective_min_count=5 retains 95917 unique words (40% of original 237658, drops 141741)\n",
      "2018-10-15 11:38:43,383 : INFO : effective_min_count=5 leaves 26215653 word corpus (99% of original 26454392, drops 238739)\n",
      "2018-10-15 11:38:43,608 : INFO : deleting the raw counts dictionary of 237658 items\n",
      "2018-10-15 11:38:43,616 : INFO : sample=0.001 downsamples 7 most-common words\n",
      "2018-10-15 11:38:43,617 : INFO : downsampling leaves estimated 25805945 word corpus (98.4% of prior 26215653)\n",
      "2018-10-15 11:38:43,953 : INFO : estimated required memory for 95917 words and 300 dimensions: 278159300 bytes\n",
      "2018-10-15 11:38:43,954 : INFO : resetting layer weights\n",
      "2018-10-15 11:38:44,862 : INFO : training model with 13 workers on 95917 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=10 window=10\n",
      "2018-10-15 11:38:45,879 : INFO : EPOCH 1 - PROGRESS: at 4.65% examples, 1155731 words/s, in_qsize 23, out_qsize 2\n",
      "2018-10-15 11:38:46,909 : INFO : EPOCH 1 - PROGRESS: at 9.66% examples, 1203120 words/s, in_qsize 26, out_qsize 0\n",
      "2018-10-15 11:38:47,911 : INFO : EPOCH 1 - PROGRESS: at 14.10% examples, 1213826 words/s, in_qsize 26, out_qsize 1\n",
      "2018-10-15 11:38:48,918 : INFO : EPOCH 1 - PROGRESS: at 18.42% examples, 1221036 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:38:49,923 : INFO : EPOCH 1 - PROGRESS: at 22.86% examples, 1218970 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:38:50,935 : INFO : EPOCH 1 - PROGRESS: at 27.74% examples, 1225043 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:38:51,947 : INFO : EPOCH 1 - PROGRESS: at 32.67% examples, 1225244 words/s, in_qsize 26, out_qsize 1\n",
      "2018-10-15 11:38:52,953 : INFO : EPOCH 1 - PROGRESS: at 37.13% examples, 1225524 words/s, in_qsize 23, out_qsize 2\n",
      "2018-10-15 11:38:53,961 : INFO : EPOCH 1 - PROGRESS: at 41.97% examples, 1226615 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:38:54,961 : INFO : EPOCH 1 - PROGRESS: at 46.74% examples, 1229570 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:38:55,973 : INFO : EPOCH 1 - PROGRESS: at 51.04% examples, 1228735 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:38:56,985 : INFO : EPOCH 1 - PROGRESS: at 56.09% examples, 1229406 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:38:57,997 : INFO : EPOCH 1 - PROGRESS: at 61.34% examples, 1230070 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:38:59,002 : INFO : EPOCH 1 - PROGRESS: at 66.13% examples, 1229241 words/s, in_qsize 24, out_qsize 3\n",
      "2018-10-15 11:39:00,004 : INFO : EPOCH 1 - PROGRESS: at 71.21% examples, 1228019 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:01,016 : INFO : EPOCH 1 - PROGRESS: at 76.50% examples, 1228387 words/s, in_qsize 26, out_qsize 2\n",
      "2018-10-15 11:39:02,038 : INFO : EPOCH 1 - PROGRESS: at 81.21% examples, 1228374 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:03,038 : INFO : EPOCH 1 - PROGRESS: at 85.98% examples, 1228140 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:04,039 : INFO : EPOCH 1 - PROGRESS: at 91.01% examples, 1227094 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:05,053 : INFO : EPOCH 1 - PROGRESS: at 95.83% examples, 1224913 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:05,826 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-10-15 11:39:05,837 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-10-15 11:39:05,838 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-10-15 11:39:05,839 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-10-15 11:39:05,848 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-10-15 11:39:05,850 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-10-15 11:39:05,868 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-10-15 11:39:05,870 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-10-15 11:39:05,871 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-10-15 11:39:05,886 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-15 11:39:05,887 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 11:39:05,894 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 11:39:05,895 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 11:39:05,896 : INFO : EPOCH - 1 : training on 26454392 raw words (25789131 effective words) took 21.0s, 1226375 effective words/s\n",
      "2018-10-15 11:39:06,915 : INFO : EPOCH 2 - PROGRESS: at 4.53% examples, 1124592 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:07,926 : INFO : EPOCH 2 - PROGRESS: at 9.24% examples, 1157677 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:08,941 : INFO : EPOCH 2 - PROGRESS: at 13.74% examples, 1180453 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:09,945 : INFO : EPOCH 2 - PROGRESS: at 17.98% examples, 1190780 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:10,948 : INFO : EPOCH 2 - PROGRESS: at 22.28% examples, 1190941 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:11,964 : INFO : EPOCH 2 - PROGRESS: at 27.03% examples, 1194760 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:12,975 : INFO : EPOCH 2 - PROGRESS: at 31.84% examples, 1196948 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:13,988 : INFO : EPOCH 2 - PROGRESS: at 36.25% examples, 1195890 words/s, in_qsize 22, out_qsize 3\n",
      "2018-10-15 11:39:14,995 : INFO : EPOCH 2 - PROGRESS: at 40.93% examples, 1198370 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:16,009 : INFO : EPOCH 2 - PROGRESS: at 45.61% examples, 1197995 words/s, in_qsize 26, out_qsize 0\n",
      "2018-10-15 11:39:17,009 : INFO : EPOCH 2 - PROGRESS: at 49.81% examples, 1199647 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:18,011 : INFO : EPOCH 2 - PROGRESS: at 54.47% examples, 1198941 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:19,013 : INFO : EPOCH 2 - PROGRESS: at 59.50% examples, 1199229 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:20,013 : INFO : EPOCH 2 - PROGRESS: at 64.42% examples, 1199066 words/s, in_qsize 26, out_qsize 1\n",
      "2018-10-15 11:39:21,019 : INFO : EPOCH 2 - PROGRESS: at 69.14% examples, 1198839 words/s, in_qsize 26, out_qsize 3\n",
      "2018-10-15 11:39:22,033 : INFO : EPOCH 2 - PROGRESS: at 74.36% examples, 1198639 words/s, in_qsize 26, out_qsize 2\n",
      "2018-10-15 11:39:23,044 : INFO : EPOCH 2 - PROGRESS: at 79.32% examples, 1198984 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:24,060 : INFO : EPOCH 2 - PROGRESS: at 83.61% examples, 1198403 words/s, in_qsize 26, out_qsize 1\n",
      "2018-10-15 11:39:25,070 : INFO : EPOCH 2 - PROGRESS: at 88.76% examples, 1199053 words/s, in_qsize 23, out_qsize 2\n",
      "2018-10-15 11:39:26,085 : INFO : EPOCH 2 - PROGRESS: at 93.72% examples, 1199349 words/s, in_qsize 23, out_qsize 2\n",
      "2018-10-15 11:39:27,101 : INFO : EPOCH 2 - PROGRESS: at 98.64% examples, 1199738 words/s, in_qsize 26, out_qsize 1\n",
      "2018-10-15 11:39:27,304 : INFO : worker thread finished; awaiting finish of 12 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-15 11:39:27,315 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-10-15 11:39:27,323 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-10-15 11:39:27,324 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-10-15 11:39:27,332 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-10-15 11:39:27,334 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-10-15 11:39:27,339 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-10-15 11:39:27,349 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-10-15 11:39:27,356 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-10-15 11:39:27,359 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-15 11:39:27,365 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 11:39:27,366 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 11:39:27,370 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 11:39:27,371 : INFO : EPOCH - 2 : training on 26454392 raw words (25789171 effective words) took 21.5s, 1201179 effective words/s\n",
      "2018-10-15 11:39:28,376 : INFO : EPOCH 3 - PROGRESS: at 4.40% examples, 1112231 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:29,389 : INFO : EPOCH 3 - PROGRESS: at 9.14% examples, 1150076 words/s, in_qsize 23, out_qsize 2\n",
      "2018-10-15 11:39:30,401 : INFO : EPOCH 3 - PROGRESS: at 13.54% examples, 1167142 words/s, in_qsize 23, out_qsize 2\n",
      "2018-10-15 11:39:31,418 : INFO : EPOCH 3 - PROGRESS: at 17.75% examples, 1174648 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:32,434 : INFO : EPOCH 3 - PROGRESS: at 22.12% examples, 1180722 words/s, in_qsize 23, out_qsize 2\n",
      "2018-10-15 11:39:33,442 : INFO : EPOCH 3 - PROGRESS: at 26.77% examples, 1183039 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:34,449 : INFO : EPOCH 3 - PROGRESS: at 31.51% examples, 1185166 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:35,450 : INFO : EPOCH 3 - PROGRESS: at 35.90% examples, 1185998 words/s, in_qsize 26, out_qsize 1\n",
      "2018-10-15 11:39:36,458 : INFO : EPOCH 3 - PROGRESS: at 40.40% examples, 1186034 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:37,470 : INFO : EPOCH 3 - PROGRESS: at 45.25% examples, 1188428 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:38,479 : INFO : EPOCH 3 - PROGRESS: at 49.39% examples, 1187181 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:39,480 : INFO : EPOCH 3 - PROGRESS: at 53.85% examples, 1185967 words/s, in_qsize 25, out_qsize 1\n",
      "2018-10-15 11:39:40,481 : INFO : EPOCH 3 - PROGRESS: at 58.83% examples, 1188867 words/s, in_qsize 26, out_qsize 1\n",
      "2018-10-15 11:39:41,482 : INFO : EPOCH 3 - PROGRESS: at 63.86% examples, 1189346 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:42,493 : INFO : EPOCH 3 - PROGRESS: at 68.44% examples, 1188729 words/s, in_qsize 22, out_qsize 3\n",
      "2018-10-15 11:39:43,510 : INFO : EPOCH 3 - PROGRESS: at 73.80% examples, 1189562 words/s, in_qsize 25, out_qsize 2\n",
      "2018-10-15 11:39:44,511 : INFO : EPOCH 3 - PROGRESS: at 78.70% examples, 1190179 words/s, in_qsize 23, out_qsize 2\n",
      "2018-10-15 11:39:45,523 : INFO : EPOCH 3 - PROGRESS: at 82.96% examples, 1189369 words/s, in_qsize 23, out_qsize 2\n",
      "2018-10-15 11:39:46,546 : INFO : EPOCH 3 - PROGRESS: at 88.05% examples, 1189562 words/s, in_qsize 23, out_qsize 2\n",
      "2018-10-15 11:39:47,558 : INFO : EPOCH 3 - PROGRESS: at 93.08% examples, 1190103 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:48,570 : INFO : EPOCH 3 - PROGRESS: at 97.90% examples, 1190222 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:48,954 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-10-15 11:39:48,958 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-10-15 11:39:48,963 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-10-15 11:39:48,964 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-10-15 11:39:48,975 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-10-15 11:39:48,990 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-10-15 11:39:48,991 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-10-15 11:39:48,995 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-10-15 11:39:48,996 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-10-15 11:39:48,999 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-15 11:39:49,015 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 11:39:49,016 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 11:39:49,019 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 11:39:49,020 : INFO : EPOCH - 3 : training on 26454392 raw words (25788399 effective words) took 21.6s, 1191475 effective words/s\n",
      "2018-10-15 11:39:50,035 : INFO : EPOCH 4 - PROGRESS: at 4.55% examples, 1137365 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:51,051 : INFO : EPOCH 4 - PROGRESS: at 9.40% examples, 1178077 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:52,061 : INFO : EPOCH 4 - PROGRESS: at 13.88% examples, 1197207 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:39:53,074 : INFO : EPOCH 4 - PROGRESS: at 18.01% examples, 1191179 words/s, in_qsize 25, out_qsize 1\n",
      "2018-10-15 11:39:54,078 : INFO : EPOCH 4 - PROGRESS: at 22.44% examples, 1196923 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:55,099 : INFO : EPOCH 4 - PROGRESS: at 27.10% examples, 1195270 words/s, in_qsize 26, out_qsize 1\n",
      "2018-10-15 11:39:56,120 : INFO : EPOCH 4 - PROGRESS: at 31.86% examples, 1194550 words/s, in_qsize 23, out_qsize 2\n",
      "2018-10-15 11:39:57,134 : INFO : EPOCH 4 - PROGRESS: at 36.41% examples, 1196088 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:39:58,135 : INFO : EPOCH 4 - PROGRESS: at 40.93% examples, 1195940 words/s, in_qsize 22, out_qsize 3\n",
      "2018-10-15 11:39:59,137 : INFO : EPOCH 4 - PROGRESS: at 45.64% examples, 1198230 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:40:00,140 : INFO : EPOCH 4 - PROGRESS: at 49.73% examples, 1196115 words/s, in_qsize 26, out_qsize 1\n",
      "2018-10-15 11:40:01,141 : INFO : EPOCH 4 - PROGRESS: at 54.47% examples, 1198156 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:40:02,157 : INFO : EPOCH 4 - PROGRESS: at 59.57% examples, 1197916 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:40:03,166 : INFO : EPOCH 4 - PROGRESS: at 64.59% examples, 1199179 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:40:04,180 : INFO : EPOCH 4 - PROGRESS: at 69.35% examples, 1198867 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:40:05,193 : INFO : EPOCH 4 - PROGRESS: at 74.50% examples, 1198186 words/s, in_qsize 22, out_qsize 3\n",
      "2018-10-15 11:40:06,207 : INFO : EPOCH 4 - PROGRESS: at 79.52% examples, 1199546 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:40:07,229 : INFO : EPOCH 4 - PROGRESS: at 83.88% examples, 1199586 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:40:08,238 : INFO : EPOCH 4 - PROGRESS: at 89.03% examples, 1199621 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:40:09,244 : INFO : EPOCH 4 - PROGRESS: at 94.06% examples, 1200906 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:40:10,246 : INFO : EPOCH 4 - PROGRESS: at 98.87% examples, 1200771 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:40:10,408 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-10-15 11:40:10,413 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-10-15 11:40:10,420 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-10-15 11:40:10,429 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-10-15 11:40:10,435 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-10-15 11:40:10,438 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-10-15 11:40:10,443 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-10-15 11:40:10,450 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-10-15 11:40:10,453 : INFO : worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-15 11:40:10,456 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-15 11:40:10,462 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 11:40:10,468 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 11:40:10,473 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 11:40:10,474 : INFO : EPOCH - 4 : training on 26454392 raw words (25788572 effective words) took 21.5s, 1202220 effective words/s\n",
      "2018-10-15 11:40:11,486 : INFO : EPOCH 5 - PROGRESS: at 4.52% examples, 1133431 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:40:12,498 : INFO : EPOCH 5 - PROGRESS: at 9.36% examples, 1178690 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:40:13,500 : INFO : EPOCH 5 - PROGRESS: at 13.71% examples, 1184543 words/s, in_qsize 26, out_qsize 1\n",
      "2018-10-15 11:40:14,518 : INFO : EPOCH 5 - PROGRESS: at 17.91% examples, 1187352 words/s, in_qsize 23, out_qsize 2\n",
      "2018-10-15 11:40:15,520 : INFO : EPOCH 5 - PROGRESS: at 22.28% examples, 1192424 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:40:16,537 : INFO : EPOCH 5 - PROGRESS: at 26.80% examples, 1186384 words/s, in_qsize 26, out_qsize 0\n",
      "2018-10-15 11:40:17,537 : INFO : EPOCH 5 - PROGRESS: at 31.47% examples, 1185074 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:40:18,540 : INFO : EPOCH 5 - PROGRESS: at 35.53% examples, 1176285 words/s, in_qsize 26, out_qsize 0\n",
      "2018-10-15 11:40:19,552 : INFO : EPOCH 5 - PROGRESS: at 40.15% examples, 1177071 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:40:20,562 : INFO : EPOCH 5 - PROGRESS: at 44.83% examples, 1177431 words/s, in_qsize 26, out_qsize 2\n",
      "2018-10-15 11:40:21,571 : INFO : EPOCH 5 - PROGRESS: at 49.01% examples, 1179182 words/s, in_qsize 26, out_qsize 1\n",
      "2018-10-15 11:40:22,571 : INFO : EPOCH 5 - PROGRESS: at 53.54% examples, 1180987 words/s, in_qsize 26, out_qsize 1\n",
      "2018-10-15 11:40:23,592 : INFO : EPOCH 5 - PROGRESS: at 58.57% examples, 1182401 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:40:24,599 : INFO : EPOCH 5 - PROGRESS: at 63.71% examples, 1184793 words/s, in_qsize 26, out_qsize 1\n",
      "2018-10-15 11:40:25,604 : INFO : EPOCH 5 - PROGRESS: at 68.35% examples, 1185647 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:40:26,632 : INFO : EPOCH 5 - PROGRESS: at 73.64% examples, 1185889 words/s, in_qsize 23, out_qsize 2\n",
      "2018-10-15 11:40:27,642 : INFO : EPOCH 5 - PROGRESS: at 78.57% examples, 1186103 words/s, in_qsize 26, out_qsize 0\n",
      "2018-10-15 11:40:28,643 : INFO : EPOCH 5 - PROGRESS: at 82.93% examples, 1187783 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:40:29,650 : INFO : EPOCH 5 - PROGRESS: at 87.89% examples, 1187542 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:40:30,663 : INFO : EPOCH 5 - PROGRESS: at 92.90% examples, 1188123 words/s, in_qsize 24, out_qsize 1\n",
      "2018-10-15 11:40:31,671 : INFO : EPOCH 5 - PROGRESS: at 97.74% examples, 1188707 words/s, in_qsize 25, out_qsize 0\n",
      "2018-10-15 11:40:32,080 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-10-15 11:40:32,083 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-10-15 11:40:32,097 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-10-15 11:40:32,099 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-10-15 11:40:32,100 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-10-15 11:40:32,112 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-10-15 11:40:32,114 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-10-15 11:40:32,121 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-10-15 11:40:32,126 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-10-15 11:40:32,129 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-15 11:40:32,131 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 11:40:32,133 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 11:40:32,141 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 11:40:32,142 : INFO : EPOCH - 5 : training on 26454392 raw words (25788795 effective words) took 21.7s, 1190430 effective words/s\n",
      "2018-10-15 11:40:32,142 : INFO : training on a 132271960 raw words (128944068 effective words) took 107.3s, 1201948 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing time for training the model:114.142902135849 seconds\n",
      "Number of words processed per second: 231765.54568863934\n",
      "Word2Vec(vocab=95917, size=300, alpha=0.025)\n",
      "Most frequent words In model:  ['said', 'new', 'year', 'like', 'people', 'time', 'years', 'york', 'city', 'percent']\n"
     ]
    }
   ],
   "source": [
    "def training(sentences):\n",
    "    #Training the model\n",
    "    tic = time.time() # Start timing\n",
    "\n",
    "    #For the Score method to work hs And negative Parameters need to be specified\n",
    "    #A good heuristic For Word vectors dimensions `size` thats frequently used is the square-root of the length of the vocabulary, after pre-processing\n",
    "\n",
    "    model = Word2Vec(sentences, # The sentences iterable can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from disk/network\n",
    "                     sg=0, #Defines the training algorithm. If 1, skip-gram is employed; otherwise, CBOW is used\n",
    "                     size=300,#Dimensionality of the feature vectors\n",
    "                     window=10,# The maximum distance between the current and predicted word within a sentence\n",
    "                     min_count=5, #Ignores all words with total frequency lower than this\n",
    "                     workers=cpu_count()-3, #Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "                     hs = 0, # int {1,0}) – If 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used.\n",
    "                     negative = 10, # If > 0, negative sampling will be used, specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "                     sample = 0.001, # The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "                     iter = 5, # (int) – Number of iterations (epochs) over the corpus. //5\n",
    "                    )\n",
    "\n",
    "    toc = time.time() # Start timing\n",
    "    computationTime = toc-tic\n",
    "    print(\"Computing time for training the model:\" +str(computationTime)+\" seconds\")\n",
    "    wordsInCorpus = sum([len(l) for l in sentences])\n",
    "    print(\"Number of words processed per second:\",wordsInCorpus/computationTime)\n",
    "    print(model)\n",
    "    print(\"Most frequent words In model: \", model.wv.index2word[:10])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = training(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discard parameters that are used in training and score. Use if you’re sure you’re done training a model.\n",
    "#If replace_word_vectors_with_normalized is set, forget the original vectors and only keep \n",
    "#the normalized ones = saves lots of memory!\n",
    "model.delete_temporary_training_data(replace_word_vectors_with_normalized=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "2018-10-15 11:40:32,520 : INFO : Pearson correlation coefficient against wordsim353.tsv: 0.5760\n",
      "2018-10-15 11:40:32,521 : INFO : Spearman rank-order correlation coefficient against wordsim353.tsv: 0.6124\n",
      "2018-10-15 11:40:32,521 : INFO : Pairs with unknown words ratio: 4.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " similarity evaluation: ((0.5759757665432773, 4.38315812303549e-31), SpearmanrResult(correlation=0.6124050885215764, pvalue=5.72660038108291e-36), 4.815864022662889)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\" similarity evaluation:\", model.wv.evaluate_word_pairs('wordsim353.tsv', restrict_vocab=50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `accuracy` (Method will be removed in 4.0.0, use self.evaluate_word_analogies() instead).\n",
      "  \n",
      "2018-10-15 11:40:33,086 : INFO : precomputing L2-norms of word weight vectors\n",
      "c:\\python3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "2018-10-15 11:40:34,467 : INFO : capital-common-countries: 57.8% (267/462)\n",
      "2018-10-15 11:40:37,137 : INFO : capital-world: 54.6% (555/1016)\n",
      "2018-10-15 11:40:37,367 : INFO : currency: 12.8% (11/86)\n",
      "2018-10-15 11:40:42,769 : INFO : city-in-state: 15.2% (313/2064)\n",
      "2018-10-15 11:40:43,662 : INFO : family: 80.1% (274/342)\n",
      "2018-10-15 11:40:45,942 : INFO : gram1-adjective-to-adverb: 14.8% (129/870)\n",
      "2018-10-15 11:40:47,267 : INFO : gram2-opposite: 16.2% (82/506)\n",
      "2018-10-15 11:40:50,742 : INFO : gram3-comparative: 70.9% (945/1332)\n",
      "2018-10-15 11:40:52,872 : INFO : gram4-superlative: 25.9% (210/812)\n",
      "2018-10-15 11:40:54,707 : INFO : gram5-present-participle: 47.0% (330/702)\n",
      "2018-10-15 11:40:58,099 : INFO : gram6-nationality-adjective: 71.3% (926/1299)\n",
      "2018-10-15 11:41:02,179 : INFO : gram7-past-tense: 57.1% (891/1560)\n",
      "2018-10-15 11:41:04,779 : INFO : gram8-plural: 58.1% (576/992)\n",
      "2018-10-15 11:41:05,885 : INFO : gram9-plural-verbs: 42.1% (177/420)\n",
      "2018-10-15 11:41:05,886 : INFO : total: 45.6% (5686/12463)\n"
     ]
    }
   ],
   "source": [
    "# Analogies\n",
    "r = model.wv.accuracy('questions-words.txt', restrict_vocab=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-15 11:41:05,895 : INFO : saving Word2Vec object under D:\\data\\nyt\\word2vecModels\\2001, separately None\n",
      "2018-10-15 11:41:05,896 : INFO : storing np array 'vectors' to D:\\data\\nyt\\word2vecModels\\2001.wv.vectors.npy\n",
      "2018-10-15 11:41:06,346 : INFO : not storing attribute vectors_norm\n",
      "2018-10-15 11:41:06,347 : INFO : not storing attribute cum_table\n",
      "2018-10-15 11:41:06,556 : INFO : saved D:\\data\\nyt\\word2vecModels\\2001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "outputFilePath = os.path.join(outputPath, year)\n",
    "model.save(outputFilePath) #binary=False saves the vectors as Textual data\n",
    "print(\"----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check to make sure models have been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-15 11:31:30,203 : INFO : loading Word2Vec object from D:\\data\\nyt\\word2vecModels\\2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-15 11:31:30,860 : INFO : loading wv recursively from D:\\data\\nyt\\word2vecModels\\2000.wv.* with mmap=None\n",
      "2018-10-15 11:31:30,861 : INFO : loading vectors from D:\\data\\nyt\\word2vecModels\\2000.wv.vectors.npy with mmap=None\n",
      "2018-10-15 11:31:30,925 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-10-15 11:31:30,925 : INFO : loading vocabulary recursively from D:\\data\\nyt\\word2vecModels\\2000.vocabulary.* with mmap=None\n",
      "2018-10-15 11:31:30,926 : INFO : loading trainables recursively from D:\\data\\nyt\\word2vecModels\\2000.trainables.* with mmap=None\n",
      "2018-10-15 11:31:30,926 : INFO : setting ignored attribute cum_table to None\n",
      "2018-10-15 11:31:30,927 : INFO : loaded D:\\data\\nyt\\word2vecModels\\2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=97229, size=300, alpha=0.025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['said',\n",
       " 'new',\n",
       " 'year',\n",
       " 'like',\n",
       " 'people',\n",
       " 'time',\n",
       " 'years',\n",
       " 'york',\n",
       " 'state',\n",
       " 'company']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity check to make sure models have been saved\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim\n",
    "\n",
    "\n",
    "print(year)\n",
    "filePath = os.path.join(outputPath, year)\n",
    "model = gensim.models.Word2Vec.load(filePath) # you can continue training with the loaded model!\n",
    "print(model)\n",
    "model.wv.index2word[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.657943069934845),\n",
       " ('policeman', 0.5610487461090088),\n",
       " ('gentleman', 0.5473320484161377),\n",
       " ('men', 0.5011500120162964),\n",
       " ('guy', 0.4981834292411804),\n",
       " ('balding', 0.4937747120857239),\n",
       " ('gruff', 0.4927574396133423),\n",
       " ('assailant', 0.4926893711090088),\n",
       " ('drifter', 0.49006587266921997),\n",
       " ('lover', 0.4891093969345093)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarityMetricsAlternative(model):\n",
    "    #listDiversityTerms=['diversity', 'diverse']\n",
    "    listDiversityTerms=['diversity']\n",
    "    \n",
    "     \n",
    "    listDemographicsTermsUpdated=[w for w in listDemographicsTerms if w in model.wv.vocab]\n",
    "    listIntellectualTermsUpdated=[w for w in listIntellectualTerms if w in model.wv.vocab]\n",
    "    \n",
    "    similarityDemographics = model.wv.n_similarity(listDiversityTerms,listDemographicsTermsUpdated)\n",
    "    similarityIntellectual = model.wv.n_similarity(listDiversityTerms,listIntellectualTermsUpdated)\n",
    "    return (similarityDemographics,similarityIntellectual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3289945841610287, 0.09222000420760601)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from commonVariables import listDemographicsTerms, listIntellectualTerms\n",
    "similarityDemographics2, similarityIntellectual2= similarityMetricsAlternative(model)\n",
    "similarityDemographics2,similarityIntellectual2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
